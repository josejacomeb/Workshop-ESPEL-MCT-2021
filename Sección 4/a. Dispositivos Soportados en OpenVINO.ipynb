{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dispositivos Soportados en OpenVINO\n",
    "OpenVINO tiene la ventaja de que tiene soporte para múltiples dispositivos de la Coompanía Intel, dispositivos como CPUs, GPUs, GNAs y VPUs son compatibles con el Software a través de la implementación de plugins. Por limitaciones de hardware, algunas capas de Deep Learning no son soportadas y tienen que ser consultadas en la documentación oficial antes de su desarrollo. Antes de explicar sobre los plugins, hay que recordar cierta terminología referentes  los modelos[1]:\n",
    "- *Formato FP32: * Formato de precisión simpre de punto flotante(32 bits)\n",
    "- *Formato FP16: * Formato de precisi'on media de punto flotante(16 bits)\n",
    "- *Formato I8: * Formato de entero con signo de 1 byte (8 bits)\n",
    "- *Quantización: * Proceso de mapera valores de entrada de un conjunto grande un proceso hacia valores de un conjunto más pequeño.  \n",
    "\n",
    "Como se puede suponer, entre menos bits utilice el modelo, mucho más rápido será su inferencia, a costa de una reducción de precisión[2], dicho proceso se llama Quantización y está plenamente soportado por OpenVINO, pero depende del hardware, como se verá en la tabla del final de este documento. \n",
    "\n",
    "## Plugin CPU [3]\n",
    "Desarrollado para proveer un alto rendimiento en inferencia de Redes Neuronales en CPUs Intel, aprovechando al máximo las instrucciones de procesador con una librería de Deep Learning optimizada de Intel y con alto paralelismo para obtener mínimo tiempo de latencia en la inferencia. Si el objetivo de desarrollo es un CPU, se provee instrucciones especializadas de Quantización y Optimizaciones para el Rendimiento[3].\n",
    "\n",
    "\n",
    "## Plugin GPU [4]\n",
    "Usa las Graphic Processing Units(GPUs) de Intel(Incluído las Integradas) a través de Software para aprovechar el máximo rendimiento de este tipo de aceleración por hardware, si el equipo dispone de varias GPUs, estas se nombraran de la forma `x={0, 1, 2, ..}`, la GPU integrada siempre tiene asignado el identificador(id) *id=0*, las consideraciones de rendimiento y capas de Inteligencia Artificial están descritas aquí [4].\n",
    "\n",
    "\n",
    "## Plugin VPU [5]\n",
    "Los Visual Processing Units(VPU) son dispositivos desarrollados con Intel, que provee la funcionalidad para la Inferencia con OpenVINO a equipos que no tienen hardware Intel, como por ejemplo el Raspberry Pi. Se pueden añadir tantos VPUs como el proceso requiera, el dispositivo tope de Gama en el segmento VPU de Intel es el Intel® Vision Accelerator Design With Intel® Movidius™ Vision Processing Unit (VPU)  que puede manejar hasta 16 dispositivos de captura al mismo tiempo. Las limitaciones y consejos, se encuentran detallados aquí[5].\n",
    "\n",
    "## Plugin VPU [5]\n",
    "Los Visual Processing Units(VPU) son dispositivos desarrollados con Intel, que provee la funcionalidad para la Inferencia con OpenVINO a equipos que no tienen hardware Intel, como por ejemplo el Raspberry Pi. Se pueden añadir tantos VPUs como el proceso requiera, el dispositivo tope de Gama en el segmento VPU de Intel es el Intel® Vision Accelerator Design With Intel® Movidius™ Vision Processing Unit (VPU)  que puede manejar hasta 16 dispositivos de captura al mismo tiempo. Las limitaciones y consejos, se encuentran detallados aquí[5].\n",
    "\n",
    "## Plugin Heterogeneo [6]\n",
    "Permite computar la inferencia en una red neuronal en varios dispositivos. El principal motivo de esta implementación es:\n",
    "- Utilizar el poder de los aceleradores de hardware(GPU, VPU, GNA) para calcular las partes más pesadas deel modelo y ejecutar las capas que no son soportadas en dispositivos de respaldo como el CPU\n",
    "- Utilizar el hardware más eficientemente durante una inferencia. \n",
    "\n",
    "Para llamar a este plugin, más adelante se conocerá como llamarlo a través de la cadena `HETERO:GPU,CPU`. Más detalles de compatiblidad en [6].\n",
    "\n",
    "## Otros Plugins\n",
    "Aparte de los plugins antes mencionados, existen otras  como el [*Plugin Multi Dispositivo*](https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_supported_plugins_MULTI.html) que se encargan de mejorar la latencia del sistema al utilizar todos los dispositivos disponibles para hacer inferencia y descargando de la inferencia a un solo dispositivo. El [*Plugin de Dispositivo Automático*](https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_supported_plugins_AUTO.html), reconoce los dispositivos disponibles de manera automática y los asigna  la ejecución de la inferencia de manera inteligente para asegurar la menor latencia del sistema(Ej. `GPU Dedicada`, `GPU Integrada`, `CPU`).\n",
    "\n",
    "## Formatos de Modelo Soportados\n",
    "\n",
    "En la siguiente tabla, se presentan las precisiones con la que la matemática de los modelos serán calculados. Considerar esto antes de aplicar Quantización al dispositivo[1].\n",
    "\n",
    "\n",
    "| Plugin     | FP32                  | FP16                  | I8           |\n",
    "|------------|-----------------------|-----------------------|--------------|\n",
    "| CPU Plugin | Soportado y preferido | Soportado             | Soportado    |\n",
    "| GPU Plugin | Soportado             | Soportado y preferido | Soportado*   |\n",
    "| VPU Plugin | No soportado          | Suportado             | No soportado |\n",
    "| GNA Plugin | Soportado             | Soportado             | No soportado |\n",
    "\n",
    "## Referencias\n",
    "- [1] Intel. (2021, Agosto 31). Supported Devices. Intel. [https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_supported_plugins_Supported_Devices.html](https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_supported_plugins_Supported_Devices.html)\n",
    "- [2] Intel. (2021, Agosto 31). Low Precision Optimization Guide. Intel. [https://docs.openvinotoolkit.org/latest/pot_docs_LowPrecisionOptimizationGuide.html](https://docs.openvinotoolkit.org/latest/pot_docs_LowPrecisionOptimizationGuide.html)\n",
    "- [3] Intel. (2021, Agosto 31). CPU Plugin. Intel. [https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_supported_plugins_CPU.html](https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_supported_plugins_CPU.html)\n",
    "- [4] Intel. (2021, Agosto 31). GPU Plugin. Intel. [https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_supported_plugins_GPU.html](https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_supported_plugins_GPU.html)\n",
    "- [5] Intel. (2021, Agosto 31). VPU Plugins. Intel. [https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_supported_plugins_VPU.html](https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_supported_plugins_VPU.html)\n",
    "- [6] Intel. (2021, Agosto 31). Heterogeneous Plugin. Intel. [https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_supported_plugins_HETERO.html](https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_supported_plugins_HETERO.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
