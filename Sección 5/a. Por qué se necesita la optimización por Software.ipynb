{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Por qué se necesita la optimización por Software?\n",
    "En procesos donde el tiempo es crítico y el hardware es limitado, la optimización de software es crucial para obtener los mejores beneficios con lo que se tiene a la mano. La *optimización por hardware* es algunas veces de implementación complicada y requiere de una inversión extra que puede sobrepasar los beneficios económicos del proyecto. \n",
    "\n",
    "## Optimización por Software\n",
    "Permite a través de cambios de código o optimización del modelo la mejora del rendimiento del sistema. Básicamente, se basan en algoritmos que reducen la complexidad computacional de los modelos. Es usualmente importante cuando los dispositivos tienen poco almacenamiento, la actualización del sistema se hace a través de una red muy pequeña y por el ahorro económico que representa la reducción de Vatios consumidos por el dispositivo para hacer inferencia.\n",
    "\n",
    "### Rendimiento de una aplicación\n",
    "El rendimiento es una medida de como un sistema realiza una tarea. Típicamente, el rendimiento está medido en términos de precisión, tiempo de inferencia o eficiencia energética del sistema. [1]\n",
    "\n",
    "### Tipos de Optimización por software\n",
    "En cualquier aplicación de Inteligencia Artifical, la mayoría del tiempo de la aplicación es consumido por la carga y el tiempo de inferencia que se demora en computar. Las siguientes técnicas han sido desarrolladas para la **Optimización del Model**\n",
    "\n",
    "#### Reducir el tamaño del modelo\n",
    "\n",
    "La principal ventaja es que reducirá el tiempo de carga del modelo y el tiempo en el que realiza la inferencia, removiendo parámetros que sean redudantes o que no tengan importancia de nuestra red.\n",
    "\n",
    "##### Cuantización\n",
    "El tamaño del modelo, se debe básicamente a los pesos de la red neuronal, que son producto del entrenamiento[2], estos pesos usualmente están en *variables formato de punto flotante de 32 bits*, usualmente el cambio del tamaño de la variable de 32 bits a 16 bits, reduciendo el tamaño del modelo y sin afectar de mayor manera a la precisión ni al rendimiento del modelo. En la siguiente imagen, se puede ver el tamaño de varios populares modelos de Deep Learning, después de aplicar la cuantización a FP16 y a INT8.\n",
    "\n",
    " ![Reducción del modelo](https://www.intel.com/content/dam/www/public/us/en/ai/images/float32-vs-compressed-models-rwd.png.rendition.intel.web.1920.1080.png)\n",
    " <div align=\"center\"><b>Fig 1.</b> Comparación del tamaño de los modelos después de aplicar cuantización [2]</div>\n",
    "\n",
    "##### Destilación del conocimiento \n",
    "Es una técnica de Compresión en el cual un modelo más pequeño es entrenado para imitar un modelo más grande que ha sido entrenado. El modelo grande, es conocido como \"Profesor\" y el pequeño como estudiante. Se ha visto que los clasificadores aprenden mucho más rápido y de mejor manera si son entrenados con las salidas de otros clasificadores como `soft labels` en vez de datos reales.\n",
    "\n",
    " ![Destilación del Conocimiento](https://i1.wp.com/miro.medium.com/max/936/1*8KqNtABnNXM527JK9UuBUQ.jpeg?resize=547%2C337&ssl=1)\n",
    " <div align=\"center\"><b>Fig 1.</b> Comparación del tamaño de los modelos después de aplicar cuantización [3]</div>\n",
    "\n",
    "\n",
    "### Reducir el número de operaciones\n",
    "\n",
    "Reducirá el tiempo para realizar inferencia por la reducción del número de operaciones y cálculos para ejecutar la red. Puede ser realizada mediante el uso de capas más eficientes y removiendo conexiones entre las neuronas en el modelo. \n",
    "\n",
    "##### Poda del Modelo (Model Pruning)\n",
    "Reduce el número de parámetros y operaciones envueltas en la computación al remover conexiones y por ende parámetros entre las redes neuronales. Reduciendo el tamaño del modelo significativamente y permitiendo evitar computación innecesaria al establecer los valores de los pesos como cero[4]. Se ha documentado que se ha logrado comprimir 6 veces el modelo con la mínima afectación a la precisión.\n",
    "\n",
    "![Poda del Modelo](https://4.bp.blogspot.com/-pPu1zJ4iKgk/XdYjR_9C_jI/AAAAAAAAA98/-YCd2tTxGlIfApkrlmmatwIo-eVVgnrIwCLcBGAsYHQ/s1600/neuralnetworklayersbeforeandafterpruning.png)\n",
    " <div align=\"center\"><b>Fig 3.</b> Comparación del tamaño de los modelos después de aplicar cuantización [4]</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "- [1] Intel. (2021, Agosto 31).What is Software Optimization and Why Does it Matter? Udacity. [https://www.udacity.com/course/intel-edge-ai-for-iot-developers-nanodegree--nd131](https://www.udacity.com/course/intel-edge-ai-for-iot-developers-nanodegree--nd131)\n",
    "- [2] Intel. (2021, Agosto 31) Reduce Application Footprint with the Latest Features in Intel® Distribution of OpenVINO™ toolkit. Intel. [https://www.intel.com/content/www/us/en/artificial-intelligence/posts/openvino-reduce-app-footprint.html#](https://www.intel.com/content/www/us/en/artificial-intelligence/posts/openvino-reduce-app-footprint.html#)\n",
    "- [3] Chris AI. (2021, Agosto 31) Introduction to Knowledge Distillation. ChrisAI [https://christineai.blog/introduction-to-knowledge-distillation/](https://christineai.blog/introduction-to-knowledge-distillation/)\n",
    "- [4] TensorFlow. (2021, Agosto 31) TensorFlow Model Optimization Toolkit — Pruning API. TensorFlow. [https://blog.tensorflow.org/2019/05/tf-model-optimization-toolkit-pruning-API.html](https://blog.tensorflow.org/2019/05/tf-model-optimization-toolkit-pruning-API.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
